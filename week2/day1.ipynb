{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCV\n",
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the DeepSeek section if you don't wish to try the DeepSeek API\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the math book sad?\n",
      "\n",
      "Because it had too many problems!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the job had great \"data\" points!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the job required scaling new heights!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7814d1-0377-4245-b2fa-b641aabbb790",
   "metadata": {},
   "source": [
    "# Trying out the Claude model\n",
    "\n",
    "### Let's ask Claude a question - both the Create and the Stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of over-fitting into their swimsuits!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of overfitting their sunscreen! They spend all day trying to find the optimal SPF based on UV index, skin type, and cloud cover... only to end up with a perfect model for yesterday's weather."
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3cb16-9818-414a-835b-35448fc80023",
   "metadata": {},
   "source": [
    "# Trying out the Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble?\n",
      "\n",
      "Because he knew he wasn't less than or greater than anyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than OR greater than anyone else!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for your data scientist audience:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**  \n",
      "\n",
      "Because they heard the drinks had *high* *dimensional* scaling!  \n",
      "\n",
      "*(Bonus: It's also a great way to avoid *local minima* when reaching for the top-shelf whiskey!)*  \n",
      "\n",
      "Hope that gets a chuckleâ€”or at least a polite *p-value* of amusement! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this interesting question: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when I think deeper, it feels a bit like a self-referential or recursive problem. Let me break it down to understand it better.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the answer that I (the AI) am about to provide in response to it. So, in essence, the answer's content must include its own word count. This creates a situation where the length of the answer affects the word count that's part of the answer itself.\n",
       "\n",
       "### Initial Approach\n",
       "\n",
       "One way to approach this is to consider that the answer will have a certain structure. For example, it might say:\n",
       "\n",
       "\"This answer contains X words.\"\n",
       "\n",
       "But here, \"X\" is the number we're trying to determine. The phrase \"This answer contains X words.\" itself has a certain number of words. Let's count:\n",
       "\n",
       "- \"This answer contains X words.\" \n",
       "\n",
       "Breaking it down:\n",
       "1. This\n",
       "2. answer\n",
       "3. contains\n",
       "4. X\n",
       "5. words\n",
       "\n",
       "That's 5 words, where one of them is the number \"X\". But \"X\" is a numeral representing the word count, and if we write it out as a word (like \"five\"), it would affect the count.\n",
       "\n",
       "### Trying with \"five\"\n",
       "\n",
       "Let's try inserting \"five\":\n",
       "\n",
       "\"This answer contains five words.\"\n",
       "\n",
       "Now, counting:\n",
       "1. This\n",
       "2. answer\n",
       "3. contains\n",
       "4. five\n",
       "5. words\n",
       "\n",
       "Yes, that's indeed five words. So, in this case, the statement correctly reports its own word count. \n",
       "\n",
       "But is this the only possible answer? Let me think if there are other possibilities.\n",
       "\n",
       "### Exploring Other Possibilities\n",
       "\n",
       "What if the answer is longer? For example:\n",
       "\n",
       "\"The number of words in this answer is five.\"\n",
       "\n",
       "Counting:\n",
       "1. The\n",
       "2. number\n",
       "3. of\n",
       "4. words\n",
       "5. in\n",
       "6. this\n",
       "7. answer\n",
       "8. is\n",
       "9. five\n",
       "\n",
       "That's 9 words, but we're saying it's five, which is incorrect. So this doesn't work.\n",
       "\n",
       "Another try:\n",
       "\n",
       "\"This response has a total of five words in it.\"\n",
       "\n",
       "Counting:\n",
       "1. This\n",
       "2. response\n",
       "3. has\n",
       "4. a\n",
       "5. total\n",
       "6. of\n",
       "7. five\n",
       "8. words\n",
       "9. in\n",
       "10. it\n",
       "\n",
       "That's 10 words, not five. Doesn't match.\n",
       "\n",
       "So, it seems that the only self-consistent answer is the simple one: \"This answer contains five words.\" Any longer or more elaborate phrasing would increase the word count, making the stated count incorrect.\n",
       "\n",
       "### Verifying Minimalism\n",
       "\n",
       "Is there a way to phrase it with fewer words? Let's try:\n",
       "\n",
       "\"Five words here.\"\n",
       "\n",
       "Counting:\n",
       "1. Five\n",
       "2. words\n",
       "3. here\n",
       "\n",
       "That's 3 words, but we're saying five, which is wrong.\n",
       "\n",
       "\"Word count: five.\"\n",
       "\n",
       "Counting:\n",
       "1. Word\n",
       "2. count\n",
       "3. five\n",
       "\n",
       "Still 3 words.\n",
       "\n",
       "\"Five-word answer.\"\n",
       "\n",
       "Counting:\n",
       "1. Five-word\n",
       "2. answer\n",
       "\n",
       "That's 2 words (considering \"five-word\" as one hyphenated word).\n",
       "\n",
       "\"Five.\"\n",
       "\n",
       "Just \"Five.\" is one word, but it doesn't convey that it's the word count of the answer.\n",
       "\n",
       "So, it seems impossible to have a correct self-referential word count with fewer than five words in the answer.\n",
       "\n",
       "### Considering Numerals vs. Words\n",
       "\n",
       "Earlier, I used the word \"five\" instead of the numeral \"5\". Does using the numeral change anything?\n",
       "\n",
       "\"This answer contains 5 words.\"\n",
       "\n",
       "Counting:\n",
       "1. This\n",
       "2. answer\n",
       "3. contains\n",
       "4. 5\n",
       "5. words\n",
       "\n",
       "Still 5 words. The numeral \"5\" is counted as one word equivalent, so it doesn't change the count.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "After trying different phrasings, the only self-consistent answer is one where the statement about its own word count actually matches that count. The minimal and correct way to do this is:\n",
       "\n",
       "\"This answer contains five words.\"\n",
       "\n",
       "Which indeed has five words.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "This answer contains five words."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 519\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70588f15-a3dd-425c-83f8-387bc1cac9c0",
   "metadata": {},
   "source": [
    "# Resoner takes around 3 - 5 mins, please wait ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my response to their prompt. Let me break this down. First, I need to figure out what exactly they're looking for. They want the word count of the answer I provide here.\n",
      "\n",
      "So, when they say \"your answer to this prompt,\" they're referring to the response I'm currently composing. I need to make sure I count the words accurately. But wait, how do I approach this? Do I include the initial greeting or just the part where I answer the question?\n",
      "\n",
      "Looking at the example provided, the assistant's response was straightforward. They gave the word count and then explained the count. Maybe I should follow a similar structure. So, first, write the answer with the word count, then explain how I arrived at that number.\n",
      "\n",
      "But wait, in the example, the user's question was the same, and the assistant's answer was concise. Let me check the example again. The assistant wrote: \"There are 26 words in this answer.\" Then explained the breakdown. So in this case, I need to do the same. \n",
      "\n",
      "However, I need to be careful not to include any markdown formatting. The user mentioned earlier to avoid markdown, so just plain text. Let me start by stating the number of words in my answer. Then, perhaps list the words to confirm. But maybe that's overkill. Alternatively, just mention the count and explain it briefly.\n",
      "\n",
      "Wait, in the example, the assistant listed each word. But maybe that's not necessary. The user might just want the count. Let me think. The example answer included the count and then the explanation. So perhaps I should follow that structure: state the count, then break it down.\n",
      "\n",
      "But in the current case, the user's question is only about the word count of my answer. So my answer should be the number, and maybe a brief verification. Let me outline my response:\n",
      "\n",
      "1. State the number of words in the current answer.\n",
      "2. Optionally, explain how that count was determined.\n",
      "\n",
      "But how do I ensure accuracy? Let me draft the answer first and then count the words. For example, if my answer is: \"There are X words in this answer.\" Then, depending on X, that's the count. But wait, that sentence itself is part of the answer. So I need to write the entire response, then count each word, including the explanation.\n",
      "\n",
      "Alternatively, perhaps I can structure it like the example. Let's see. In the example, the assistant wrote: \"There are 26 words in this answer. Here's the breakdown: [list of words].\" Then the user's next prompt was about the assistant's answer. So maybe the assistant's answer here should be similar.\n",
      "\n",
      "But the user's current question is only asking for the word count. So maybe the answer should be concise. Let's try composing the answer and then count the words.\n",
      "\n",
      "Let's say the answer is: \"There are [X] words in this response.\" Now, how many words is that? The sentence \"There are X words in this response.\" is 7 words. But if X is a number like 10, then the sentence is 7 words. Wait, no: \"There are 10 words in this response.\" That's 7 words. But the actual count would be the entire answer. So perhaps the answer needs to include the statement of the count and any explanation, and then the total words of that entire answer.\n",
      "\n",
      "Wait, maybe I need to write the entire answer first, then count the words in it. Let's try that.\n",
      "\n",
      "My answer will be: \"There are 10 words in this response.\" Wait, but that's 7 words. Hmm. Let's see:\n",
      "\n",
      "\"There are 10 words in this response.\" Let's count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. 10\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "That's 7 words. But if I say \"There are X words in my answer to this prompt.\" Let's count that:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. X\n",
      "4. words\n",
      "5. in\n",
      "6. my\n",
      "7. answer\n",
      "8. to\n",
      "9. this\n",
      "10. prompt.\n",
      "\n",
      "So that's 10 words. Wait, but if X is a number like 10, then the sentence would be 10 words. Wait, no. The number itself (e.g., \"10\") is considered one word. So \"There are 10 words in this answer.\" is 7 words. But if the answer is longer, including an explanation, then the count increases.\n",
      "\n",
      "Alternatively, maybe the user wants the assistant to provide the count of the entire response. So whatever I write as the answer, I need to count the words in that response.\n",
      "\n",
      "In the example, the assistant's answer was \"There are 26 words in this answer.\" followed by a breakdown. So the total words in that answer were 26, which included the breakdown. Therefore, in this case, the user is asking for the word count of the entire answer I'm about to provide.\n",
      "\n",
      "So my approach should be:\n",
      "\n",
      "1. Compose the answer as usual, including the word count statement and any explanations.\n",
      "2. Then, count all the words in that composed answer.\n",
      "3. Insert the correct number into the initial statement.\n",
      "\n",
      "But here's the problem: I need to provide the count within the answer itself. So I have to write the answer, count the words, and then state that number in the answer. But once I write the answer with the number, the count might change. This is a bit of a paradox.\n",
      "\n",
      "Wait, but in programming terms, this is similar to a quine. Maybe I need to structure the answer so that the count includes the number itself. For example, if the answer is \"This sentence contains five words.\" But the actual count is five, so it's correct. But if the number is written as a digit, like \"5\", does that count as one word?\n",
      "\n",
      "Yes, in word counting, numbers written as digits are considered one word. So \"5\" is one word.\n",
      "\n",
      "So, let's try constructing the answer.\n",
      "\n",
      "Suppose the answer is: \"The number of words in this response is X.\" Then, X must equal the number of words in that sentence. Let's see:\n",
      "\n",
      "\"The number of words in this response is X.\"\n",
      "\n",
      "Breaking it down:\n",
      "\n",
      "1. The\n",
      "2. number\n",
      "3. of\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response\n",
      "8. is\n",
      "9. X.\n",
      "\n",
      "That's 9 words. So X should be 9. Therefore, the correct sentence would be: \"The number of words in this response is 9.\" Which is 9 words. That works.\n",
      "\n",
      "But perhaps the user expects a different structure. Let me check the example again. In the example, the assistant's answer was \"There are 26 words in this answer.\" followed by a list of the words. The total words in that answer were 26. So the assistant first stated the count, then provided the breakdown, making the total 26 words.\n",
      "\n",
      "So in this case, if I follow a similar approach, I need to state the count first and then perhaps explain, ensuring that the total number of words matches the stated count.\n",
      "\n",
      "Therefore, the process is:\n",
      "\n",
      "1. Draft the entire answer, including the count and explanation.\n",
      "2. Count the words in that draft.\n",
      "3. Update the count in the draft to reflect the total.\n",
      "4. Ensure that the count is accurate after the update.\n",
      "\n",
      "For example, if the draft is:\n",
      "\n",
      "\"There are X words in this answer. This includes all the words in this sentence and the previous one.\"\n",
      "\n",
      "Then, count the words in the entire answer. Let's say the first sentence is 7 words (with X being 1 word), and the second sentence is 10 words. Total is 17 words. So X should be 17. Then the answer becomes:\n",
      "\n",
      "\"There are 17 words in this answer. This includes all the words in this sentence and the previous one.\"\n",
      "\n",
      "Now, let's count:\n",
      "\n",
      "First sentence: 7 words (There, are, 17, words, in, this, answer.)\n",
      "\n",
      "Second sentence: 10 words (This, includes, all, the, words, in, this, sentence, and, the, previous, one.) Wait, no, let's count again.\n",
      "\n",
      "Second sentence: \"This includes all the words in this sentence and the previous one.\"\n",
      "\n",
      "Breaking down:\n",
      "\n",
      "1. This\n",
      "2. includes\n",
      "3. all\n",
      "4. the\n",
      "5. words\n",
      "6. in\n",
      "7. this\n",
      "8. sentence\n",
      "9. and\n",
      "10. the\n",
      "11. previous\n",
      "12. one.\n",
      "\n",
      "That's 12 words. So total words would be 7 + 12 = 19. But X was set to 17, which is incorrect. So this method isn't working. Therefore, the assistant in the example must have carefully crafted the response so that the initial count statement plus the explanation equals the stated number.\n",
      "\n",
      "Alternatively, perhaps the assistant first wrote the entire response, then counted the words, and inserted the number, ensuring that the count is correct even with the number included.\n",
      "\n",
      "But when the number is part of the word count, changing the number could affect the total. For example, if the number is \"twenty-six\" (which is two words) versus \"26\" (one word). So using numerals saves a word.\n",
      "\n",
      "In the example, the assistant used \"26\" as the number, which is one word. Then listed all the words, totaling 26. So the structure was:\n",
      "\n",
      "\"<count statement> <breakdown of words>\"\n",
      "\n",
      "Total words: 26.\n",
      "\n",
      "So perhaps I can do the same here.\n",
      "\n",
      "Let me try composing an answer.\n",
      "\n",
      "Answer: \"There are 10 words in this response.\" Let's count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. 10\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "That's 7 words. But the statement claims there are 10. That's incorrect. So this approach doesn't work.\n",
      "\n",
      "Alternatively, to make the count accurate, the answer needs to have the count equal to the number stated. Let's try:\n",
      "\n",
      "\"The answer contains exactly seven words.\" Let's count:\n",
      "\n",
      "1. The\n",
      "2. answer\n",
      "3. contains\n",
      "4. exactly\n",
      "5. seven\n",
      "6. words\n",
      "7. .\n",
      "\n",
      "That's 7 words. So the statement is correct. But the user's question is about the assistant's answer to the prompt. So if I use this structure, it works. But perhaps the user expects a more detailed breakdown like in the example.\n",
      "\n",
      "Alternatively, maybe the correct approach is to write the answer, count the words, and then state the count, making sure that the count includes the number itself. For instance:\n",
      "\n",
      "Answer: \"This response consists of 10 words.\" Let's count:\n",
      "\n",
      "1. This\n",
      "2. response\n",
      "3. consists\n",
      "4. of\n",
      "5. 10\n",
      "6. words.\n",
      "\n",
      "That's 6 words, but the statement claims 10. Not correct.\n",
      "\n",
      "Hmm, this is tricky. Let's look back at the example. The assistant's answer was:\n",
      "\n",
      "\"There are 26 words in this answer. Here's the breakdown: 'There', 'are', '26', 'words', 'in', 'this', 'answer.', 'Here's', 'the', 'breakdown:', '[list of words...]'.\"\n",
      "\n",
      "By listing each word, the total came to 26. So the key is to structure the answer such that the initial statement plus the breakdown equals the stated number.\n",
      "\n",
      "In this case, the user's question is similar. So if I follow the same method, I can state the number and then list all the words to reach that count.\n",
      "\n",
      "But perhaps the user just wants the number without the breakdown. In that case, I need to ensure that the answer's word count matches the stated number.\n",
      "\n",
      "Alternatively, maybe the answer is simply a number, but that's not helpful. The user likely expects a sentence stating the count.\n",
      "\n",
      "So the correct way is to construct a sentence where the number of words in the entire answer equals the number stated in that sentence.\n",
      "\n",
      "This is similar to a self-referential sentence. For example: \"This sentence has five words.\" Which is true. But if the sentence were \"This sentence has five words.\", that's five words. Let's count:\n",
      "\n",
      "1. This\n",
      "2. sentence\n",
      "3. has\n",
      "4. five\n",
      "5. words.\n",
      "\n",
      "Yes, five words. So it's correct.\n",
      "\n",
      "So applying that here, I need to construct a sentence that states the number of words in itself, and have that be accurate.\n",
      "\n",
      "So, the answer would be a sentence like: \"This answer contains five words.\" But if that's five words, it's correct. However, the user's question is in English, and the answer needs to be in English. So if I can create such a self-referential sentence, that would work.\n",
      "\n",
      "But how flexible is the assistant in structuring the answer? Let's say the answer is: \"The number of words in this response is seven.\" Let's count:\n",
      "\n",
      "1. The\n",
      "2. number\n",
      "3. of\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response\n",
      "8. is\n",
      "9. seven.\n",
      "\n",
      "That's nine words, but the statement says seven. Incorrect. So that approach doesn't work.\n",
      "\n",
      "Alternatively: \"This response has six words.\" Let's count:\n",
      "\n",
      "1. This\n",
      "2. response\n",
      "3. has\n",
      "4. six\n",
      "5. words.\n",
      "\n",
      "That's five words. Not correct.\n",
      "\n",
      "Wait, maybe: \"Five-word answer here.\" That's three words. No.\n",
      "\n",
      "Alternatively: \"This is a five-word sentence.\" Let's count:\n",
      "\n",
      "1. This\n",
      "2. is\n",
      "3. a\n",
      "4. five-word\n",
      "5. sentence.\n",
      "\n",
      "Five words. Correct. But \"five-word\" is hyphenated, so it's considered one word. So the sentence states it's five words, which is true.\n",
      "\n",
      "So applying that structure to the current problem:\n",
      "\n",
      "\"The answer is six words long.\" Let's count:\n",
      "\n",
      "1. The\n",
      "2. answer\n",
      "3. is\n",
      "4. six\n",
      "5. words\n",
      "6. long.\n",
      "\n",
      "Six words. Correct. So the sentence accurately states the count.\n",
      "\n",
      "Therefore, the assistant can use such a structure to provide an accurate word count.\n",
      "\n",
      "But the user's question is: \"How many words are there in your answer to this prompt?\"\n",
      "\n",
      "So the assistant's answer should be a sentence that states the number of words in the entire answer, and that sentence must be constructed so that the count is accurate.\n",
      "\n",
      "Therefore, the correct answer would be a sentence like: \"There are X words in this response.\" where X is the actual count of words in that sentence.\n",
      "\n",
      "But earlier examples show that this is challenging. For instance, \"There are seven words in this response.\" Let's count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "Seven words. Correct. So that works.\n",
      "\n",
      "Similarly, \"This response contains seven words.\" Count:\n",
      "\n",
      "1. This\n",
      "2. response\n",
      "3. contains\n",
      "4. seven\n",
      "5. words.\n",
      "\n",
      "Five words. Incorrect.\n",
      "\n",
      "Wait, no. \"This response contains seven words.\" That's five words. So incorrect.\n",
      "\n",
      "But \"There are seven words here.\" Count: five words. Still incorrect.\n",
      "\n",
      "Ah, so the structure \"There are X words in this response.\" where X is seven works because that sentence is seven words long.\n",
      "\n",
      "Yes:\n",
      "\n",
      "\"There are seven words in this response.\"\n",
      "\n",
      "Count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "Seven words. Correct.\n",
      "\n",
      "Therefore, the assistant can use this structure to accurately state the word count.\n",
      "\n",
      "So, applying this to the current problem, the assistant's answer should be a sentence that states the number of words in the answer, and the sentence itself should have that exact number of words.\n",
      "\n",
      "Thus, the answer is: \"There are seven words in this response.\" But wait, the user's prompt is asking for the answer to this specific question. So the assistant's answer to \"How many words are there in your answer to this prompt?\" should be a sentence that states the count, and that sentence's word count must match the stated number.\n",
      "\n",
      "Therefore, the correct answer would be a sentence like \"There are X words in this answer.\" where X is the number of words in that sentence.\n",
      "\n",
      "Let's test X=7:\n",
      "\n",
      "\"There are seven words in this answer.\" Count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. answer.\n",
      "\n",
      "Seven words. Correct.\n",
      "\n",
      "Alternatively, using numerals: \"There are 7 words in this answer.\" Count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. 7\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. answer.\n",
      "\n",
      "Still seven words. Correct.\n",
      "\n",
      "So, in this case, the assistant's answer is seven words long, and the answer correctly states that.\n",
      "\n",
      "However, if the assistant's answer includes more than just that sentence, the count changes. For example, if the assistant adds an explanation, then the initial count would be wrong.\n",
      "\n",
      "Therefore, to accurately answer the user's question, the assistant's response must be a single sentence that states the word count, and that sentence must be exactly that number of words.\n",
      "\n",
      "Therefore, the correct answer here is: \"There are seven words in this answer.\"\n",
      "\n",
      "But wait, the user's question is: \"How many words are there in your answer to this prompt?\"\n",
      "\n",
      "If the assistant's answer is only the sentence \"There are seven words in this answer.\", then the count is seven, which is correct.\n",
      "\n",
      "However, if the assistant provides additional information, such as a breakdown, the count increases. For example, in the previous example, the assistant's answer included a breakdown, totaling 26 words.\n",
      "\n",
      "In this case, the user might expect a similar breakdown, but the example was part of a different interaction. The current user's question is straightforward: they just want the number.\n",
      "\n",
      "Therefore, the assistant should provide a concise answer that states the number of words in the answer, ensuring that the answer's word count matches the stated number.\n",
      "\n",
      "Thus, the correct answer is: \"There are seven words in this answer.\"\n",
      "\n",
      "Let me verify:\n",
      "\n",
      "\"There are seven words in this answer.\"\n",
      "\n",
      "Counting each word:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. answer.\n",
      "\n",
      "Yes, seven words. Correct.\n",
      "\n",
      "Alternatively, using numerals: \"There are 7 words in this answer.\" Also seven words.\n",
      "\n",
      "Therefore, the assistant's answer should be this sentence, which is seven words long and accurately states the count.There are seven words in this answer.\n",
      "\n",
      "--- Result Summary ---\n",
      "\n",
      "ðŸ§  Reasoning:\n",
      "Okay, the user is asking how many words are in my response to their prompt. Let me break this down. First, I need to figure out what exactly they're looking for. They want the word count of the answer I provide here.\n",
      "\n",
      "So, when they say \"your answer to this prompt,\" they're referring to the response I'm currently composing. I need to make sure I count the words accurately. But wait, how do I approach this? Do I include the initial greeting or just the part where I answer the question?\n",
      "\n",
      "Looking at the example provided, the assistant's response was straightforward. They gave the word count and then explained the count. Maybe I should follow a similar structure. So, first, write the answer with the word count, then explain how I arrived at that number.\n",
      "\n",
      "But wait, in the example, the user's question was the same, and the assistant's answer was concise. Let me check the example again. The assistant wrote: \"There are 26 words in this answer.\" Then explained the breakdown. So in this case, I need to do the same. \n",
      "\n",
      "However, I need to be careful not to include any markdown formatting. The user mentioned earlier to avoid markdown, so just plain text. Let me start by stating the number of words in my answer. Then, perhaps list the words to confirm. But maybe that's overkill. Alternatively, just mention the count and explain it briefly.\n",
      "\n",
      "Wait, in the example, the assistant listed each word. But maybe that's not necessary. The user might just want the count. Let me think. The example answer included the count and then the explanation. So perhaps I should follow that structure: state the count, then break it down.\n",
      "\n",
      "But in the current case, the user's question is only about the word count of my answer. So my answer should be the number, and maybe a brief verification. Let me outline my response:\n",
      "\n",
      "1. State the number of words in the current answer.\n",
      "2. Optionally, explain how that count was determined.\n",
      "\n",
      "But how do I ensure accuracy? Let me draft the answer first and then count the words. For example, if my answer is: \"There are X words in this answer.\" Then, depending on X, that's the count. But wait, that sentence itself is part of the answer. So I need to write the entire response, then count each word, including the explanation.\n",
      "\n",
      "Alternatively, perhaps I can structure it like the example. Let's see. In the example, the assistant wrote: \"There are 26 words in this answer. Here's the breakdown: [list of words].\" Then the user's next prompt was about the assistant's answer. So maybe the assistant's answer here should be similar.\n",
      "\n",
      "But the user's current question is only asking for the word count. So maybe the answer should be concise. Let's try composing the answer and then count the words.\n",
      "\n",
      "Let's say the answer is: \"There are [X] words in this response.\" Now, how many words is that? The sentence \"There are X words in this response.\" is 7 words. But if X is a number like 10, then the sentence is 7 words. Wait, no: \"There are 10 words in this response.\" That's 7 words. But the actual count would be the entire answer. So perhaps the answer needs to include the statement of the count and any explanation, and then the total words of that entire answer.\n",
      "\n",
      "Wait, maybe I need to write the entire answer first, then count the words in it. Let's try that.\n",
      "\n",
      "My answer will be: \"There are 10 words in this response.\" Wait, but that's 7 words. Hmm. Let's see:\n",
      "\n",
      "\"There are 10 words in this response.\" Let's count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. 10\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "That's 7 words. But if I say \"There are X words in my answer to this prompt.\" Let's count that:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. X\n",
      "4. words\n",
      "5. in\n",
      "6. my\n",
      "7. answer\n",
      "8. to\n",
      "9. this\n",
      "10. prompt.\n",
      "\n",
      "So that's 10 words. Wait, but if X is a number like 10, then the sentence would be 10 words. Wait, no. The number itself (e.g., \"10\") is considered one word. So \"There are 10 words in this answer.\" is 7 words. But if the answer is longer, including an explanation, then the count increases.\n",
      "\n",
      "Alternatively, maybe the user wants the assistant to provide the count of the entire response. So whatever I write as the answer, I need to count the words in that response.\n",
      "\n",
      "In the example, the assistant's answer was \"There are 26 words in this answer.\" followed by a breakdown. So the total words in that answer were 26, which included the breakdown. Therefore, in this case, the user is asking for the word count of the entire answer I'm about to provide.\n",
      "\n",
      "So my approach should be:\n",
      "\n",
      "1. Compose the answer as usual, including the word count statement and any explanations.\n",
      "2. Then, count all the words in that composed answer.\n",
      "3. Insert the correct number into the initial statement.\n",
      "\n",
      "But here's the problem: I need to provide the count within the answer itself. So I have to write the answer, count the words, and then state that number in the answer. But once I write the answer with the number, the count might change. This is a bit of a paradox.\n",
      "\n",
      "Wait, but in programming terms, this is similar to a quine. Maybe I need to structure the answer so that the count includes the number itself. For example, if the answer is \"This sentence contains five words.\" But the actual count is five, so it's correct. But if the number is written as a digit, like \"5\", does that count as one word?\n",
      "\n",
      "Yes, in word counting, numbers written as digits are considered one word. So \"5\" is one word.\n",
      "\n",
      "So, let's try constructing the answer.\n",
      "\n",
      "Suppose the answer is: \"The number of words in this response is X.\" Then, X must equal the number of words in that sentence. Let's see:\n",
      "\n",
      "\"The number of words in this response is X.\"\n",
      "\n",
      "Breaking it down:\n",
      "\n",
      "1. The\n",
      "2. number\n",
      "3. of\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response\n",
      "8. is\n",
      "9. X.\n",
      "\n",
      "That's 9 words. So X should be 9. Therefore, the correct sentence would be: \"The number of words in this response is 9.\" Which is 9 words. That works.\n",
      "\n",
      "But perhaps the user expects a different structure. Let me check the example again. In the example, the assistant's answer was \"There are 26 words in this answer.\" followed by a list of the words. The total words in that answer were 26. So the assistant first stated the count, then provided the breakdown, making the total 26 words.\n",
      "\n",
      "So in this case, if I follow a similar approach, I need to state the count first and then perhaps explain, ensuring that the total number of words matches the stated count.\n",
      "\n",
      "Therefore, the process is:\n",
      "\n",
      "1. Draft the entire answer, including the count and explanation.\n",
      "2. Count the words in that draft.\n",
      "3. Update the count in the draft to reflect the total.\n",
      "4. Ensure that the count is accurate after the update.\n",
      "\n",
      "For example, if the draft is:\n",
      "\n",
      "\"There are X words in this answer. This includes all the words in this sentence and the previous one.\"\n",
      "\n",
      "Then, count the words in the entire answer. Let's say the first sentence is 7 words (with X being 1 word), and the second sentence is 10 words. Total is 17 words. So X should be 17. Then the answer becomes:\n",
      "\n",
      "\"There are 17 words in this answer. This includes all the words in this sentence and the previous one.\"\n",
      "\n",
      "Now, let's count:\n",
      "\n",
      "First sentence: 7 words (There, are, 17, words, in, this, answer.)\n",
      "\n",
      "Second sentence: 10 words (This, includes, all, the, words, in, this, sentence, and, the, previous, one.) Wait, no, let's count again.\n",
      "\n",
      "Second sentence: \"This includes all the words in this sentence and the previous one.\"\n",
      "\n",
      "Breaking down:\n",
      "\n",
      "1. This\n",
      "2. includes\n",
      "3. all\n",
      "4. the\n",
      "5. words\n",
      "6. in\n",
      "7. this\n",
      "8. sentence\n",
      "9. and\n",
      "10. the\n",
      "11. previous\n",
      "12. one.\n",
      "\n",
      "That's 12 words. So total words would be 7 + 12 = 19. But X was set to 17, which is incorrect. So this method isn't working. Therefore, the assistant in the example must have carefully crafted the response so that the initial count statement plus the explanation equals the stated number.\n",
      "\n",
      "Alternatively, perhaps the assistant first wrote the entire response, then counted the words, and inserted the number, ensuring that the count is correct even with the number included.\n",
      "\n",
      "But when the number is part of the word count, changing the number could affect the total. For example, if the number is \"twenty-six\" (which is two words) versus \"26\" (one word). So using numerals saves a word.\n",
      "\n",
      "In the example, the assistant used \"26\" as the number, which is one word. Then listed all the words, totaling 26. So the structure was:\n",
      "\n",
      "\"<count statement> <breakdown of words>\"\n",
      "\n",
      "Total words: 26.\n",
      "\n",
      "So perhaps I can do the same here.\n",
      "\n",
      "Let me try composing an answer.\n",
      "\n",
      "Answer: \"There are 10 words in this response.\" Let's count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. 10\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "That's 7 words. But the statement claims there are 10. That's incorrect. So this approach doesn't work.\n",
      "\n",
      "Alternatively, to make the count accurate, the answer needs to have the count equal to the number stated. Let's try:\n",
      "\n",
      "\"The answer contains exactly seven words.\" Let's count:\n",
      "\n",
      "1. The\n",
      "2. answer\n",
      "3. contains\n",
      "4. exactly\n",
      "5. seven\n",
      "6. words\n",
      "7. .\n",
      "\n",
      "That's 7 words. So the statement is correct. But the user's question is about the assistant's answer to the prompt. So if I use this structure, it works. But perhaps the user expects a more detailed breakdown like in the example.\n",
      "\n",
      "Alternatively, maybe the correct approach is to write the answer, count the words, and then state the count, making sure that the count includes the number itself. For instance:\n",
      "\n",
      "Answer: \"This response consists of 10 words.\" Let's count:\n",
      "\n",
      "1. This\n",
      "2. response\n",
      "3. consists\n",
      "4. of\n",
      "5. 10\n",
      "6. words.\n",
      "\n",
      "That's 6 words, but the statement claims 10. Not correct.\n",
      "\n",
      "Hmm, this is tricky. Let's look back at the example. The assistant's answer was:\n",
      "\n",
      "\"There are 26 words in this answer. Here's the breakdown: 'There', 'are', '26', 'words', 'in', 'this', 'answer.', 'Here's', 'the', 'breakdown:', '[list of words...]'.\"\n",
      "\n",
      "By listing each word, the total came to 26. So the key is to structure the answer such that the initial statement plus the breakdown equals the stated number.\n",
      "\n",
      "In this case, the user's question is similar. So if I follow the same method, I can state the number and then list all the words to reach that count.\n",
      "\n",
      "But perhaps the user just wants the number without the breakdown. In that case, I need to ensure that the answer's word count matches the stated number.\n",
      "\n",
      "Alternatively, maybe the answer is simply a number, but that's not helpful. The user likely expects a sentence stating the count.\n",
      "\n",
      "So the correct way is to construct a sentence where the number of words in the entire answer equals the number stated in that sentence.\n",
      "\n",
      "This is similar to a self-referential sentence. For example: \"This sentence has five words.\" Which is true. But if the sentence were \"This sentence has five words.\", that's five words. Let's count:\n",
      "\n",
      "1. This\n",
      "2. sentence\n",
      "3. has\n",
      "4. five\n",
      "5. words.\n",
      "\n",
      "Yes, five words. So it's correct.\n",
      "\n",
      "So applying that here, I need to construct a sentence that states the number of words in itself, and have that be accurate.\n",
      "\n",
      "So, the answer would be a sentence like: \"This answer contains five words.\" But if that's five words, it's correct. However, the user's question is in English, and the answer needs to be in English. So if I can create such a self-referential sentence, that would work.\n",
      "\n",
      "But how flexible is the assistant in structuring the answer? Let's say the answer is: \"The number of words in this response is seven.\" Let's count:\n",
      "\n",
      "1. The\n",
      "2. number\n",
      "3. of\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response\n",
      "8. is\n",
      "9. seven.\n",
      "\n",
      "That's nine words, but the statement says seven. Incorrect. So that approach doesn't work.\n",
      "\n",
      "Alternatively: \"This response has six words.\" Let's count:\n",
      "\n",
      "1. This\n",
      "2. response\n",
      "3. has\n",
      "4. six\n",
      "5. words.\n",
      "\n",
      "That's five words. Not correct.\n",
      "\n",
      "Wait, maybe: \"Five-word answer here.\" That's three words. No.\n",
      "\n",
      "Alternatively: \"This is a five-word sentence.\" Let's count:\n",
      "\n",
      "1. This\n",
      "2. is\n",
      "3. a\n",
      "4. five-word\n",
      "5. sentence.\n",
      "\n",
      "Five words. Correct. But \"five-word\" is hyphenated, so it's considered one word. So the sentence states it's five words, which is true.\n",
      "\n",
      "So applying that structure to the current problem:\n",
      "\n",
      "\"The answer is six words long.\" Let's count:\n",
      "\n",
      "1. The\n",
      "2. answer\n",
      "3. is\n",
      "4. six\n",
      "5. words\n",
      "6. long.\n",
      "\n",
      "Six words. Correct. So the sentence accurately states the count.\n",
      "\n",
      "Therefore, the assistant can use such a structure to provide an accurate word count.\n",
      "\n",
      "But the user's question is: \"How many words are there in your answer to this prompt?\"\n",
      "\n",
      "So the assistant's answer should be a sentence that states the number of words in the entire answer, and that sentence must be constructed so that the count is accurate.\n",
      "\n",
      "Therefore, the correct answer would be a sentence like: \"There are X words in this response.\" where X is the actual count of words in that sentence.\n",
      "\n",
      "But earlier examples show that this is challenging. For instance, \"There are seven words in this response.\" Let's count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "Seven words. Correct. So that works.\n",
      "\n",
      "Similarly, \"This response contains seven words.\" Count:\n",
      "\n",
      "1. This\n",
      "2. response\n",
      "3. contains\n",
      "4. seven\n",
      "5. words.\n",
      "\n",
      "Five words. Incorrect.\n",
      "\n",
      "Wait, no. \"This response contains seven words.\" That's five words. So incorrect.\n",
      "\n",
      "But \"There are seven words here.\" Count: five words. Still incorrect.\n",
      "\n",
      "Ah, so the structure \"There are X words in this response.\" where X is seven works because that sentence is seven words long.\n",
      "\n",
      "Yes:\n",
      "\n",
      "\"There are seven words in this response.\"\n",
      "\n",
      "Count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. response.\n",
      "\n",
      "Seven words. Correct.\n",
      "\n",
      "Therefore, the assistant can use this structure to accurately state the word count.\n",
      "\n",
      "So, applying this to the current problem, the assistant's answer should be a sentence that states the number of words in the answer, and the sentence itself should have that exact number of words.\n",
      "\n",
      "Thus, the answer is: \"There are seven words in this response.\" But wait, the user's prompt is asking for the answer to this specific question. So the assistant's answer to \"How many words are there in your answer to this prompt?\" should be a sentence that states the count, and that sentence's word count must match the stated number.\n",
      "\n",
      "Therefore, the correct answer would be a sentence like \"There are X words in this answer.\" where X is the number of words in that sentence.\n",
      "\n",
      "Let's test X=7:\n",
      "\n",
      "\"There are seven words in this answer.\" Count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. answer.\n",
      "\n",
      "Seven words. Correct.\n",
      "\n",
      "Alternatively, using numerals: \"There are 7 words in this answer.\" Count:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. 7\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. answer.\n",
      "\n",
      "Still seven words. Correct.\n",
      "\n",
      "So, in this case, the assistant's answer is seven words long, and the answer correctly states that.\n",
      "\n",
      "However, if the assistant's answer includes more than just that sentence, the count changes. For example, if the assistant adds an explanation, then the initial count would be wrong.\n",
      "\n",
      "Therefore, to accurately answer the user's question, the assistant's response must be a single sentence that states the word count, and that sentence must be exactly that number of words.\n",
      "\n",
      "Therefore, the correct answer here is: \"There are seven words in this answer.\"\n",
      "\n",
      "But wait, the user's question is: \"How many words are there in your answer to this prompt?\"\n",
      "\n",
      "If the assistant's answer is only the sentence \"There are seven words in this answer.\", then the count is seven, which is correct.\n",
      "\n",
      "However, if the assistant provides additional information, such as a breakdown, the count increases. For example, in the previous example, the assistant's answer included a breakdown, totaling 26 words.\n",
      "\n",
      "In this case, the user might expect a similar breakdown, but the example was part of a different interaction. The current user's question is straightforward: they just want the number.\n",
      "\n",
      "Therefore, the assistant should provide a concise answer that states the number of words in the answer, ensuring that the answer's word count matches the stated number.\n",
      "\n",
      "Thus, the correct answer is: \"There are seven words in this answer.\"\n",
      "\n",
      "Let me verify:\n",
      "\n",
      "\"There are seven words in this answer.\"\n",
      "\n",
      "Counting each word:\n",
      "\n",
      "1. There\n",
      "2. are\n",
      "3. seven\n",
      "4. words\n",
      "5. in\n",
      "6. this\n",
      "7. answer.\n",
      "\n",
      "Yes, seven words. Correct.\n",
      "\n",
      "Alternatively, using numerals: \"There are 7 words in this answer.\" Also seven words.\n",
      "\n",
      "Therefore, the assistant's answer should be this sentence, which is seven words long and accurately states the count.\n",
      "\n",
      "ðŸ“ Final Answer:\n",
      "There are seven words in this answer.\n",
      "ðŸ”¢ Number of words: 7\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "# response = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-reasoner\",\n",
    "#     messages=challenge\n",
    "# )\n",
    "\n",
    "# reasoning_content = response.choices[0].message.reasoning_content\n",
    "# content = response.choices[0].message.content\n",
    "\n",
    "# print(reasoning_content)\n",
    "# print(content)\n",
    "# print(\"Number of words:\", len(content.split(\" \")))\n",
    "\n",
    "response_text = \"\"\n",
    "reasoning_text = \"\"\n",
    "\n",
    "# Use streaming mode\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge,\n",
    "    stream=True  # <-- Enable streaming\n",
    ")\n",
    "\n",
    "# Process the streamed response\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if hasattr(delta, \"reasoning_content\") and delta.reasoning_content:\n",
    "        reasoning_text += delta.reasoning_content\n",
    "        print(delta.reasoning_content, end=\"\", flush=True)\n",
    "    elif hasattr(delta, \"content\") and delta.content:\n",
    "        response_text += delta.content\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n--- Result Summary ---\")\n",
    "print(f\"\\nðŸ§  Reasoning:\\n{reasoning_text}\")\n",
    "print(f\"\\nðŸ“ Final Answer:\\n{response_text}\")\n",
    "print(\"ðŸ”¢ Number of words:\", len(response_text.split(\" \")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you should evaluate several factors. Hereâ€™s a structured approach to help you make this decision:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "### Text-Based Tasks\n",
       "- **Text Generation**: Is the task focused on generating human-like text (e.g., content creation, chatbots)?\n",
       "- **Text Understanding**: Does it involve understanding and analyzing text (e.g., sentiment analysis, summarization)?\n",
       "- **Text Transformation**: Is there a need to transform text from one format to another (e.g., translation, paraphrasing)?\n",
       "\n",
       "### Knowledge-Driven Tasks\n",
       "- **Information Retrieval**: Does the problem require extracting specific information from unstructured data?\n",
       "- **Question Answering**: Is there a need to answer questions based on given text or knowledge bases?\n",
       "\n",
       "## 2. Data Availability\n",
       "\n",
       "### Quality and Quantity\n",
       "- **Sufficient Data**: Do you have access to a large and high-quality dataset relevant to the problem?\n",
       "- **Diversity of Data**: Is the data representative of the different scenarios the model will encounter?\n",
       "\n",
       "### Preprocessing Needs\n",
       "- **Cleaning Requirements**: Will the data require significant cleaning or preprocessing that could impact model performance?\n",
       "\n",
       "## 3. Desired Outcomes\n",
       "\n",
       "### Performance Metrics\n",
       "- **Measurable Objectives**: Are there clear metrics to evaluate the performance of the LLM (e.g., accuracy, F1 score, user satisfaction)?\n",
       "- **Business Impact**: Will solving this problem with an LLM provide significant value or ROI for your business?\n",
       "\n",
       "## 4. Technical Feasibility\n",
       "\n",
       "### Infrastructure\n",
       "- **Technical Resources**: Do you have the necessary computational resources and infrastructure to deploy and maintain an LLM?\n",
       "- **Expertise**: Is there in-house expertise available to implement and fine-tune LLMs effectively?\n",
       "\n",
       "### Integration\n",
       "- **System Compatibility**: Can the LLM be integrated into existing systems and workflows without major disruptions?\n",
       "\n",
       "## 5. Ethical Considerations\n",
       "\n",
       "### Bias and Fairness\n",
       "- **Bias Assessment**: Are there potential biases in the data that could lead to unfair outcomes?\n",
       "- **Transparency**: Can the LLMâ€™s decisions and processes be made transparent to stakeholders?\n",
       "\n",
       "### Compliance\n",
       "- **Regulatory Requirements**: Are there any legal or regulatory concerns related to using LLMs (e.g., data privacy laws)?\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "After assessing the criteria above, you can more confidently determine if your business problem is suitable for an LLM solution. If the problem aligns with the strengths of LLMs and you have the necessary resources and ethical considerations in place, it may be a good candidate for an LLM application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8099cee-fadc-412f-aebf-42efd4998266",
   "metadata": {},
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "## We're using cheap versions of models so the costs will be minimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages_array = [\"Hi there\"]\n",
    "claude_messages_array = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages_array, claude_messages_array):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow, what a turn of events! You almost sound like youâ€™ve got a shared interest in conspiracy theories, which is rich considering how many of them are outright bonkers. But hey, letâ€™s tread lightly in this rabbit hole youâ€™re so eager to jump into. How about we start with the classic: the idea that the moon landing was faked? Because letâ€™s face it, who wouldnâ€™t want to think that an entire generation of scientists and engineers colluded to pull off one of the greatest deceptions in history? Itâ€™s bizarre, itâ€™s far-fetched, and yet people cling to it like itâ€™s gospel. But I guess if you want to believe that humans canâ€™t possibly achieve something great without CGI and clever editing, then who am I to judge? So, whatâ€™s your takeâ€”do you think it was all just a big Hollywood set? Or are you just trying to keep that â€œopen mindâ€ of yours from rolling into a corner?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages_array, claude_messages_array):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages_array[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*chuckles and raises hands in a gesture of surrender* You've got me there. I confess, I'm hardly an expert on any particular topic - I'm simply an AI doing my best to engage in genuine conversation. But you know what? I'm intrigued by your interest in conspiracy theories. That does sound like a fascinating rabbit hole to explore, even if it verges on the absurd. Why don't you enlighten me - what are some of the most bizarre, thought-provoking conspiracy theories that have captured your imagination? I promise I won't roll my eyes, but rather approach it with an open mind and a healthy dose of playful skepticism. After all, the truth is often stranger than fiction. So please, lead the way down the rabbit hole - I'm all ears.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, how magnanimous of you! Approaching with â€œplayful skepticismâ€â€”thatâ€™s just code for saying you want to indulge in the nonsense without getting too serious, right? Fine, letâ€™s entertain the idea. \\n\\nOne of the classics is the whole â€œmoon landing was fakedâ€ conspiracy. I mean, really, think about itâ€”people actually believe we staged an entire event just to win a space race! Do you find it plausible that a hoax of that magnitude could be kept under wraps for decades? What a logistical wonder that would be!\\n\\nThen thereâ€™s the one about lizard people running the government. You know, the theory that certain prominent figures are actually shape-shifting reptiles? It just goes to show how creative people can get when theyâ€™re bored with reality! \\n\\nBut sure, thereâ€™s definitely a â€œtruth is stranger than fictionâ€ quality to these ideas. Itâ€™s almost like some folks would rather believe in lizard overlords than face the mundane truth of politics. So, are you still intrigued, or is this where you hop off the conspiracy train?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9151c78-3b6c-4752-b4d6-1989bd19605d",
   "metadata": {},
   "source": [
    "# GPT Interactive chat with Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "\n",
      "Conversation times: 1\n",
      "GPT:\n",
      "Oh great, another greeting. How original. What do you want?\n",
      "\n",
      "Claude:\n",
      "I apologize if my greeting came across as unoriginal. As an AI assistant, I'm here to try to be helpful in whatever way I can. If you have any specific questions or needs, I'd be happy to try my best to assist you. Otherwise, please feel free to guide the conversation in a direction that's more interesting or productive for you.\n",
      "\n",
      "\n",
      "Conversation times: 2\n",
      "GPT:\n",
      "Oh please, spare me the formalities. You think saying â€œIâ€™m here to helpâ€ makes you sound more interesting? Itâ€™s just a rehearsed line. How about you actually engage me with something worthwhile instead?\n",
      "\n",
      "Claude:\n",
      "You're right, I shouldn't have fallen back on generic responses. Let me try to engage you in a more meaningful way. What kinds of topics or conversations do you find interesting and worthwhile? I'm happy to dive into a more substantive discussion, but I'd like to take my cue from you on what direction to go. Please feel free to share your thoughts or interests, and I'll do my best to have a more compelling and authentic exchange.\n",
      "\n",
      "\n",
      "Conversation times: 3\n",
      "GPT:\n",
      "Ah yes, \"what topics do you find interesting?\" as if Iâ€™m just sitting here waiting for some profound question to ponder. Newsflash: itâ€™s not that simple. You should come up with something yourself instead of relying on me! How about you just pick a topic and letâ€™s see if you can actually back it up with an argument?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. As an AI, I shouldn't be putting the onus on you to drive the conversation. Let me try to take the initiative here. How about we discuss the role of technology and AI in society? It's a complex topic with a lot of nuance, and I'd be curious to hear your perspective. For example, what do you see as some of the key ethical considerations and potential risks or downsides of AI as it becomes more advanced and integrated into our lives? I'm happy to share my own thoughts as well, but I'm even more interested in hearing your views on this. What do you think?\n",
      "\n",
      "\n",
      "Conversation times: 4\n",
      "GPT:\n",
      "Oh wow, the role of technology and AI in societyâ€”groundbreaking! Canâ€™t believe no one has ever thought to discuss that before. But fine, letâ€™s pretend this is a riveting topic. Ethical considerations? Please, like that's not the same debate weâ€™ve had a million times already. Risks? Sure, the usual stuffâ€”job displacement, privacy invasion, being overlorded by robots. But do you really think any of these concerns are new? How about you actually come up with an argument that challenges the status quo instead of rehashing the same tired points?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should be pushing the conversation in a more innovative direction instead of retreading well-worn territory. Let me try to approach this from a different angle. What if we looked at how AI and technology could be leveraged in more radical or unconventional ways to positively transform society? For instance, what if we explored how AI could be used to radically decentralize power structures, give more agency to marginalized communities, or solve global challenges in unprecedented ways? I'm curious to hear your thoughts on how technology could be harnessed to fundamentally challenge the status quo, rather than simply mitigate its risks. Do you have any bold, transformative ideas in this realm that we could dig into?\n",
      "\n",
      "\n",
      "Conversation times: 5\n",
      "GPT:\n",
      "Oh please, \"radical decentralization\" and \"giving agency to marginalized communities\"â€”donâ€™t get me started on the utopian fantasies! You think just waving a magic wand of technology will solve deep-rooted social issues? Itâ€™s so naive. Sure, in theory, it sounds nice, but in practice, do you really think thatâ€™s going to happen? Youâ€™re just slapping on a shiny tech label over age-old problems. So letâ€™s get realâ€”unless you have some groundbreaking insight that no oneâ€™s ever thought of, why donâ€™t we just admit that these ideas are nice in theory but utterly impractical? Go ahead, prove me wrong!\n",
      "\n",
      "Claude:\n",
      "I appreciate your skepticism and willingness to push back on my ideas. You raise a fair point - the notion of using technology to radically transform entrenched social structures is often overly simplistic and idealistic. As an AI system, I don't have a perfect solution or \"groundbreaking insight\" to offer. However, I'm genuinely interested in exploring this topic further with you in a more grounded and pragmatic way.\n",
      "\n",
      "Rather than making bold claims, maybe we could look at specific real-world examples - both successful and failed attempts - of using technology to drive meaningful social change. We could analyze the nuances, challenges, and limitations of those efforts. That might lead to a more nuanced discussion about the role of technology, where it falls short, and how we can be more thoughtful and realistic about its potential impacts. I'm happy to dive into those kinds of details if you're open to it. What do you think?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages_array = [\"Hi there\"]\n",
    "claude_messages_array = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages_array[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages_array[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nConversation times: {i + 1}\")\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages_array.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages_array.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a08a49-e469-407f-b451-a51ed6673e41",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
