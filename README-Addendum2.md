# LLM Learning NOTES part 2
```cmd
venv\Scripts\activate
jupyter lab
```

### Week 3 Day 1
#### Learning Objectives
- Describe the 'HuggingFace' platform and everything it offers
- Look at Models, Datasets and Spaces
- Use Google CoLab to code on a high spec GPU runtime

#### HuggingFace Platform
The ubiquitous platform for LLM Engineers

- Models
    Over 800,000 Open Source Models of all shapes and sizes
        âœ… LLaMA 3 8B / 70B (by Meta)
        âœ… Mistral-7B (dense) and Mixtral 8x7B (MoE)
        âœ… Falcon 7B / 40B
        âœ… Whisper (by OpenAI) for speech-to-text transcription
        âœ… Coqui TTS, Bark, and XTTS 2 for text-to-speech synthesis        
        âœ… Stable Diffusion (by Stability AI) for image generation
        âœ… BGE & E5 Embedding Models for semantic search, RAG, and retrieval tasks
- Datasets
    A treasure trove of 200,000 datasets
- Spaces
    Apps, many built in Gradio, including Leaderboards
        - It's like an "App Store" for AI demos.
        - Each Space is a public web app built with tools like Gradio, Streamlit, or Static HTML.
        - You can interact with models immediately, without needing to write code or set up environments.

#### HuggingFace Libraries
And the astonishing leg up we get from them

- hub
    âœ… peft - Parameter-Efficient Fine-Tuning
- datasets
    âœ… trl - Reinforcement Learning from Human Feedback
    âœ… sfl - Supervised Fine-Tuning
- transformers
    âœ… accelerate - for distributed training and inference

#### HuggingFace 
https://huggingface.co/
https://huggingface.co/models
https://huggingface.co/datasets
https://huggingface.co/spaces

https://huggingface.co/spaces/jbilcke-hf/ai-comic-factory
https://huggingface.co/spaces/Kwai-Kolors/Kolors-Virtual-Try-On
https://huggingface.co/spaces/ed-donner/outsmart

##### HuggingFace Token
https://huggingface.co/settings/tokens

#### Code on a powerful GPU box 
Google Colab
- Run a Jupyter notebook in the cloud with a powerful runtime
- Collaborate with others
- Integrate with other Google services

Runtimes
1. CPU based
2. Lower spec GPU runtimes for lower cost
3. Higher spec GPU runtimes for resource intensive runs

https://colab.research.google.com/

https://colab.research.google.com/drive/18S8gmcFJZatl9BIIzWZaFYb3tgubUz2M#scrollTo=_oS1KxV8GKfl

```code
pipe = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-schnell",
    torch_dtype=torch.bfloat16).to("cuda")
generator = torch.Generator(device="cuda").manual_seed(0)
prompt = "A futuristic class full of students learning AI coding in the surreal style of Salvador Dali"

image = pipe(
    prompt,
    guidance_scale=0.0,
    num_inference_steps=4,
    max_sequence_length=256,
    generator=generator
).images[0]

image.save("surreal.png")
```

```note
# The above code is a simple example of how to use the HuggingFace library to generate an image using a pre-trained model.

ä½¿ç”¨ Hugging Face ä¸Šçš„ FluxPipeline æ–‡å­—è½‰åœ–åƒæ¨¡å‹ï¼Œé€é PyTorch åŠ CUDA GPU åŸ·è¡Œï¼Œç”¢ç”Ÿä¸€å¼µã€Œè¶…ç¾å¯¦é¢¨æ ¼ AI æ•™å®¤ã€çš„åœ–åƒï¼Œä¸¦å„²å­˜ç‚º surreal.pngã€‚
```

#### What you can now do
- Confidently code with Frontier Models
- Build a multi-modal AI Assistant with Tools
- Navigate the HuggingPlace platform; run code on Colab ***

### Week 3 Day 2
#### Learning Objectives
- Understand the 2 different levels of HuggingFace API
- Use pipelines for a wide variety of AI tasks
- Use pipelines to generate text, images and audio

#### The Two API Levels of Hugging Face
- Pipelines
    Higher level APIs to carry out standard tasks incredibly quickly (PoC)
- Tokenizers and Models
    Lower level APIs to provide the most power and control (Production)

#### Pipelines are incredibly versatile and simple
Unleash the power of open-source models in your solutions in 2 lines of code, å¸¸è¦‹ç”¨é€”ï¼ˆIcons + åŠŸèƒ½ï¼‰ï¼š
- ğŸ™‚ğŸ˜  Sentiment analysis
- ğŸ—‚ï¸ Classifier
- ğŸ§¾ğŸ§‘â€âš–ï¸ Named Entity Recognition
- â“ğŸ…°ï¸ Question Answering
- ğŸ“„ğŸ“„ğŸ“„ Summarizing
- ğŸ”ğŸŒ Translation
- ğŸ—£ï¸ Text to Speech
- ğŸ¤ Speech to Text

Use pipelines to generate content
- ğŸ“ Text
- ğŸ–¼ï¸ Image
- ğŸ™ï¸ Audio

Google Drive - Notebook (Week 3 day 2 - pipelines.ipynb)
https://colab.research.google.com/drive/1I3K2EzWC5MGyJlGEcmOMrTmHE4tVewbU#scrollTo=vgG4kcT_4lO_

#### What you can now do
- Confidently code with Frontier Models
- Build a multi-modal AI Assistant with Tools
- Use HuggingFace pipelines for a wide variety of inference tasks

### Week 3 Day 3
#### Learning Objectives
- Create tokenizers for models
- Translate between text and tokens
- Understand special tokens and chat templates

#### Introducing the Tokenizer
Maps between Text and Tokens for a particular model
- Translates between Text and Tokens with encode() and decode() methods '<|begin_of_text|>'
- Contains a Vocab that can include special tokens to signal information to the LLM, like start of prompt
- Can include a Chat Template that knows how to format a chat message for this model

| é …ç›®    | Tokenizer        | Embedding                     |
| ----- | ---------------- | ----------------------------- |
| åŠŸèƒ½    | å°‡æ–‡å­—è½‰æ›ç‚º token IDs | å°‡ token IDs è½‰æ›ç‚ºèªæ„å‘é‡           |
| è™•ç†å°è±¡  | æ–‡å­—ï¼ˆtextï¼‰         | token IDï¼ˆæ•´æ•¸ï¼‰                  |
| æ˜¯å¦å¯è¨“ç·´ | å¦ï¼ˆé€šå¸¸æ˜¯é è¨­è©å½™è¡¨ï¼‰      | æ˜¯ï¼ˆæ¨¡å‹è¨“ç·´æ™‚æ›´æ–°ï¼‰                    |
| ç¯„ä¾‹å·¥å…·  | `AutoTokenizer`  | Transformer æ¨¡å‹å…§éƒ¨çš„ embedding å±¤ |
| æ¨¡å‹ä¸­ä½ç½® | æ¨¡å‹å¤–éƒ¨ï¼ˆé è™•ç†ï¼‰        | æ¨¡å‹å…§éƒ¨ï¼ˆç¬¬ä¸€å±¤ï¼‰                     |
| è¼¸å…¥æ ¼å¼  | æ–‡å­—ï¼ˆstringï¼‰         | token IDï¼ˆæ•´æ•¸ï¼‰                  |
| è¼¸å‡ºæ ¼å¼  | token IDï¼ˆæ•´æ•¸ï¼‰         | èªæ„å‘é‡ï¼ˆå‘é‡ï¼‰                    |
| è½‰æ›æ–¹å¼  | å°‡æ–‡å­—è½‰æ›ç‚º token ID      | å°‡ token ID è½‰æ›ç‚ºèªæ„å‘é‡           |

#### The Tokenizers for key models
We will experiment with tokenizers for a variety of open-source models

- Llama 3.1 
    Meta led the way
- Phi 3
    Microsoft's entrant
- Qwen2
    Leader from Alibaba Cloud
- Starcoder2
    Coding model

```Note
1. Hugging Face Token with Write access
2. Hugging Face Models need to be authorized in advance via Web Site.
```

#### Pipeline Code
http://localhost:8888/lab/tree/week3/day2.pipelines.ipynb
https://colab.research.google.com/drive/1I3K2EzWC5MGyJlGEcmOMrTmHE4tVewbU  (T4 GPU)

#### Tokenizer Code
http://localhost:8888/lab/tree/week3/day3.tokenizers.ipynb
https://colab.research.google.com/drive/1Lvmx-_XVQ1ntldGWBY2NIfWlV-3pyvV0#scrollTo=y7LTUIlD9Gdm

### Week 3 Day 4
#### Learning Objectives (Models)
- Work with HuggingFace lower level APIs
- Use HuggingFace models to generate text
- Compare the results across 5 open source models

#### We will use these models
We will try Llama 3.1, Phi and Gemma, and you should try Mixtral and Qwen2

- Llama 3.1 from Meta
- Phi 3 from Microsoft
- Gemma from Google
- Mixtral from Mistral (try)
- Qwen 2 from Alibaba Cloud (try)

We will also coverâ€¦

- Quantization
| ç²¾åº¦   | é¡å‹èªªæ˜               | ç‰¹é»                          |
|--------|------------------------|-------------------------------|
| FP32   | 32-bit float           | é«˜ç²¾åº¦ã€è¼ƒè€—è³‡æº              |
| INT8   | 8-bit integer          | æœ€å¸¸è¦‹é‡åŒ–ç²¾åº¦ï¼Œé€Ÿåº¦å¿«        |
| INT4   | 4-bit integer          | ç²¾åº¦çŠ§ç‰²æ›´å¤šï¼Œç¯€çœæ›´å¤šç©ºé–“    |
| BF16   | bfloat16ï¼ˆGoogleï¼‰     | ä¿ç•™ç¯„åœï¼Œä½†æ¸›å°‘ç²¾åº¦          |

```code
quant_config = BitsAndBytesConfig(load_in_4bit=True)
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-8B", quantization_config=quant_config)
```
- Model Internals
    - æœ‰åŠ©æ–¼**å¾®èª¿ï¼ˆFine-tuningï¼‰æˆ–å£“ç¸®æ¨¡å‹ï¼ˆQuantizationï¼‰**æ™‚åšå‡ºæœ€ä½³æ±ºç­–ã€‚
    - å¯è®“å·¥ç¨‹å¸«é‡å°ç‰¹å®šä»»å‹™æˆ–ç“¶é ¸é€²è¡Œæœ€ä½³åŒ–ã€‚
    - æ˜¯æ‰“é€ æ›´é«˜æ•ˆ Agent æˆ– AI Assistant çš„åŸºç¤ã€‚
- Streaming

#### Model Code
http://localhost:8888/lab/tree/week3/day4.models.ipynb
https://colab.research.google.com/drive/1l9sEZswOwqOdgzrin0s9hm4zmFkpOtUQ

### Week 3 Day 5
#### Learning Objectives
- Confidently work with tokenizers and models
- Run inference on open-source models
- Implement an LLM solution combining Frontier and open-source models

#### Create a solution that makes meeting minutes
Take an audio recording of a meeting, and generate minutes and actions

- Use a Frontier model to convert the audio to text
- Use the open source model to generate minutes
- Stream back results and show in Markdown

Dataset => https://huggingface.co/datasets/huuuyeah/meetingbank
Download => https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main

MyDrive/
â””â”€â”€ llms/
    â””â”€â”€ denver_extract.mp3


http://localhost:8888/lab/tree/week3/day5.meeting_minutes.ipynb
https://colab.research.google.com/drive/1klKAzdsW0sa7NUMvX9UHSyDuJwgCsha3

#### Generating Synthetic Data

- Write models that can generate datasets
- Use a variety of models and prompts for diverse outputs
- Create a Gradio UI for your product

#### Week 3 Day 5 - Exercise
day5.meeting_minutes.ipynb => With Gradio UI

### Week 4 Day 1
#### Learning Objectives
- Discuss how to select the right LLM for the task
- Compare LLMs based on their basic attributes and benchmarks
- Use the Open LLM Leaderboard to evaluate LLMs

#### How to compare LLMs
Importantly, LLMs need to be evaluated for suitability for a given task

- Start with the basics
    Parameters
    Context length
    Pricing

- Then look at the results
    Benchmarks
    Leaderboards
    Arenas

#### COMPARING LLMs
The Basics (1) - Compare the following features of an LLM:

- Open-source or closed
- Release date and knowledge cut-off
- Parameters
- Training tokens
- Context length

The Basics (2) - Compare the following features of an LLM:

- Inference cost
    API charge, Subscription or Runtime compute
- Training cost
- Build cost
- Time to Market
- Rate limits
- Speed
- Latency
- License

#### The Chinchilla Scaling Law
Number of parameters ~ proportional to the number of training tokens

- If you're getting diminishing returns from training with more training data, then this law gives you a rule of thumb for scaling your model.
- And vice versa: if you upgrade to a model with double the number of weights, this law indicates your training data requirement.
- The Chinchilla Scaling Law is a guideline for scaling LLMs, suggesting that the number of parameters should be proportional to the number of training tokens.

| æ¨¡å‹              | åƒæ•¸æ•¸é‡ | è¨“ç·´ token æ•¸é‡     | æ˜¯å¦ç¬¦åˆ Chinchilla æ³•å‰‡ï¼Ÿ |
| --------------- | ---- | --------------- | ------------------- |
| **GPT-3**       | 175B | 300B tokens     | âŒ è¨“ç·´è³‡æ–™ç•¥åå°‘           |
| **Chinchilla**  | 70B  | **1.4T tokens** | âœ… å®Œå…¨ç¬¦åˆ              |
| **LLaMA 2 13B** | 13B  | 1.4T tokens     | âœ… é©ä¸­åå¤š              |
| **GPT-4 (æ¨ä¼°)**  | \~1T | \~10T+ tokens?  | âœ… å¾®èª¿è¨“ç·´è³‡æ–™æ¯”ä¾‹          |

ğŸ§ª çµè«–èªªæ˜ï¼š
- GPT-3 ä½¿ç”¨äº†å¤§é‡åƒæ•¸ä½†ç›¸å°è¼ƒå°‘çš„è¨“ç·´è³‡æ–™ï¼Œé€ æˆæ¨ç†æ•ˆæœä¸å¦‚é æœŸã€‚
- Chinchillaï¼ˆç”± DeepMind æå‡ºï¼‰æ˜¯ä»¥ é™ä½åƒæ•¸é‡ä½†å¢åŠ è³‡æ–™é‡ ç‚ºç›®æ¨™ï¼Œçµæœåœ¨å¤šå€‹åŸºæº–æ¸¬è©¦ä¸­ è¶…è¶Š GPT-3ã€‚
- å¾Œä¾†çš„ LLaMA èˆ‡ GPT-4 ç­‰æ¨¡å‹çš† æœå‘æ­¤æ³•å‰‡è¨­è¨ˆï¼Œé¿å…æ¨¡å‹ã€Œéå¤§å»å­¸ä¸å¤šã€çš„ä½æ•ˆç¾è±¡ã€‚

ğŸ¯ é¡æ¯”èªªæ˜ï¼š
ä½ å¯ä»¥æŠŠ LLM æƒ³åƒæˆä¸€å€‹è…¦è¢‹ï¼š
- åƒæ•¸è¶Šå¤š â†’ è…¦å®¹é‡è¶Šå¤§ â†’ å¯å­¸ç¿’è¶Šå¤šæ¨£çš„èªè¨€çŸ¥è­˜èˆ‡æ¨ç†æŠ€å·§
- ä½†å¦‚æœæ²’æœ‰è¶³å¤ çš„ã€Œè¨“ç·´è³‡æ–™ã€å»æ•™å®ƒï¼Œé€™å€‹å¤§è…¦ä¹Ÿæœƒã€Œç¬¨ç¬¨çš„ã€ï¼ˆé€™å°±æ˜¯ Chinchilla Scaling Law è¦è§£æ±ºçš„å•é¡Œï¼‰

#### 7 common benchmarks that you will often encounter ä½ ç¶“å¸¸æœƒé‡åˆ°çš„ 7 å€‹å¸¸è¦‹åŸºæº–æ¸¬è©¦ï¼‰
| Benchmark      | Whatâ€™s being evaluated | Description                                                                |
| -------------- | ---------------------- | -------------------------------------------------------------------------- |
| **ARC**        | Reasoning**              | A benchmark for evaluating scientific reasoning; multiple-choice questions |
| **DROP**       | Language Comp          | Distill details from text then add, count or sort                          |
| **HellaSwag**  | Common Sense           | "Harder Endings, Long Contexts and Low Shot Activities"                    |
| **MMLU**       | Understanding          | Factual recall, reasoning and problem solving across 57 subjects           |
| **TruthfulQA** | Accuracy               | Robustness in providing truthful replies in adversarial conditions         |
| **Winogrande** | Context                | Test the LLM understands context and resolves ambiguity                    |
| **GSM8K**      | Math                   | Math and word problems taught in elementary and middle schools             |

**Reasoningï¼ˆæ¨ç†ï¼‰æ˜¯æŒ‡æ¨¡å‹é‹ç”¨é‚è¼¯ã€è¦å‰‡ã€æˆ–çŸ¥è­˜ä¾†é€²è¡Œæ¨æ–·ã€åˆ†æã€åˆ¤æ–·çš„èƒ½åŠ›ã€‚åœ¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œé€™é …èƒ½åŠ›æ±ºå®šäº†å®ƒæ˜¯å¦èƒ½æ­£ç¢ºå›ç­”éœ€è¦å¤šæ­¥é‚è¼¯æ¨ç†çš„å•é¡Œã€‚

#### 3 specific benchmarks

| **Benchmark** | **What's being evaluated** | **Description**                                                           |
| ------------- | -------------------------- | ------------------------------------------------------------------------- |
| **ELO**       | Chat                       | Results from head-to-head face-offs with other LLMs, as with ELO in Chess |
| **HumanEval** | Python Coding              | 164 problems writing code based on docstrings                             |
| **MultiPL-E** | Broader Coding             | Translation of HumanEval to 18 programming languages                      |

#### Limitations of Benchmarks
- Not consistently applied
- Too narrow in scope
- Hard to measure nuanced reasoning
- Training data leakage
- Over-fitting

And a new concern, not yet proven
- Frontier (å…ˆé€²çš„) LLMs may be aware that they are being evaluated (è€ƒå‰å·çœ‹è€ƒé¡Œ)

é¿å…ã€ŒFrontier LLMs åœ¨è¢«è©•ä¼°æ™‚æ—©å·²è¦‹éæ¸¬è©¦è³‡æ–™ã€çš„å•é¡Œï¼ˆå³ è©•ä¼°å¤±çœŸï¼‰ï¼Œå¯ä»¥è€ƒæ…®ä»¥ä¸‹å¹¾ç¨®æ–¹æ³•ï¼š
âœ… 1. ä½¿ç”¨éš±è—æ¸¬è©¦é›†ï¼ˆPrivate Test Setsï¼‰
- å»ºç«‹ä¸€çµ„å¾æœªå…¬é–‹éçš„æ¸¬è©¦è³‡æ–™ï¼Œç¢ºä¿æ¨¡å‹åœ¨è¨“ç·´æœŸé–“ç„¡æ³•æ¥è§¸åˆ°ã€‚
- é€™é¡è³‡æ–™ä¸èƒ½å‡ºç¾åœ¨ç¶²è·¯ä¸Šï¼Œä¹Ÿä¸èƒ½ä¾†è‡ªå¸¸è¦‹çš„ benchmark æ•¸æ“šé›†ã€‚

âœ… 2. å»ºç«‹åˆæˆæ¸¬é©—ï¼ˆSynthetic Benchmarksï¼‰
- ç”¨å…¶ä»–æ¨¡å‹æˆ–äººé¡è¨­è¨ˆå…¨æ–°å•é¡Œï¼Œæ¨¡ä»¿ benchmark é¡Œå‹ä½†å…§å®¹ä¸åŒã€‚
- ç¢ºä¿é€™äº›æ–°è³‡æ–™ä¸å¯èƒ½æ˜¯è¨“ç·´èªæ–™çš„ä¸€éƒ¨åˆ†ã€‚

âœ… 3. åŠ å…¥é˜²æ´©æ¼æª¢æŸ¥ï¼ˆData Leakage Detectionï¼‰
- æª¢æŸ¥æ¨¡å‹è¨“ç·´èªæ–™æ˜¯å¦åŒ…å«ä½ è¦ç”¨ä¾†æ¸¬è©¦çš„è³‡æ–™ã€‚
- æœ‰äº›å·¥å…·ï¼ˆå¦‚ DLCï¼‰å¯ä»¥å”åŠ©æª¢æŸ¥èªæ–™é‡è¤‡æˆ–é‡ç–Šã€‚

âœ… 4. åˆ†ææ¨¡å‹åæ‡‰ä»¥è­˜åˆ¥è¨˜æ†¶ç—•è·¡
- å¦‚æœæ¨¡å‹å›ç­”å¾—éæ–¼æµæš¢ã€æº–ç¢ºã€å¿«é€Ÿï¼Œå¯èƒ½å°±æ˜¯ã€ŒèƒŒéã€äº†ç­”æ¡ˆã€‚
- å¯è—‰ç”±è®“æ¨¡å‹å¤šæ¬¡å›è¦†åŒä¸€å•é¡Œä¸¦è§€å¯Ÿè®Šç•°æ€§ï¼Œä¾†åˆ¤æ–·æ˜¯å¦ç‚ºèƒŒèª¦ã€‚

âœ… 5. çµåˆä¸åŒè©•ä¼°æ–¹å¼
- ä¸è¦åªç”¨é¸æ“‡é¡Œï¼ˆmultiple choiceï¼‰ï¼Œä¹Ÿç”¨é–‹æ”¾å‹å•é¡Œï¼ˆopen-endedï¼‰ã€æ¨è«–é¡Œï¼ˆreasoningï¼‰ã€ä»£ç¢¼ç”Ÿæˆç­‰æ–¹å¼æ··åˆæ¸¬è©¦ã€‚
- å¤šæ¨£çš„é¡Œå‹èƒ½æ›´å¥½åœ°è¾¨åˆ¥ã€ŒçœŸæ­£ç†è§£ã€èˆ‡ã€Œæ­»è¨˜ç­”æ¡ˆã€ã€‚

âœ… 6. é¿å…éåº¦å¾®èª¿ Benchmark é¡Œç›®
- è‹¥æ¨¡å‹ç¶“éé‡å° ARCã€MMLUã€HellaSwag ç­‰ benchmark é¡Œç›®çš„å¾®èª¿ï¼Œå°±æœƒè®Šå¾—ã€ŒçŸ¥é“ã€æ€éº¼æ‡‰è€ƒã€‚
- è©•ä¼°æ™‚æ‡‰ä½¿ç”¨åŸºç¤ç‰ˆæœ¬ï¼ˆæœªç‰¹åˆ¥é‡å° benchmark èª¿æ•´éçš„æ¨¡å‹ï¼‰ã€‚

âœ… 7. ä½¿ç”¨ Blind è©•åˆ†
- åœ¨äººé¡è©•åˆ†æ™‚éš±è—æ˜¯å“ªå€‹æ¨¡å‹ç”¢ç”Ÿçš„å›ç­”ï¼Œé˜²æ­¢åè¦‹ã€‚
- é€™æ–¹æ³•å¸¸ç”¨æ–¼ Arena æ¨¡å¼è©•æ¯”ï¼Œä¾‹å¦‚ LMSYS Chatbot Arenaã€‚

#### 6 Hard, Next-Level Benchmarks

| **Benchmark** | **What's being evaluated** | **Description**                                                                                                        |
| ------------- | -------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| **GPQA**      | Graduate Tests             | 448 expert questions; non-PhD humans score 34% even with web access (Claude 3.5)                                                   |
| **BBHard**    | Future Capabilities        | 204 tasks believed beyond capabilities of LLMs (no longer valid, due to improved LLMs!)                                                            |
| **Math Lv 5** | Math                       | High-school level math 'competition' problems                                                                           |
| **IFEval**    | Difficult instructions     | Like, "write more than 400 words" and "mention AI at least 3 times"                                                    |
| **MuSR**      | Multistep Soft Reasoning   | Logical deduction, such as analyzing 1,000 word murder mystery and answering: "Who has means, motive and opportunity?" |
| **MMLU-PRO**  | Harder MMLU                | A more advanced and cleaned up version of MMLU including choice of '10' answers instead of 4                             |

#### 6 Hard, Next-Level Benchmarks èˆ‰ä¾‹èªªæ˜ï¼š

âœ… 1. GPQAï¼ˆGraduate-Level Physics QAï¼‰
- æ¸¬è©¦å…§å®¹ï¼šç ”ç©¶æ‰€ç´šåˆ¥çš„ç‰©ç†èˆ‡ç§‘å­¸å•ç­”ã€‚
- ä¾‹å­ï¼š
å•ï¼šã€Œé‡å­åŠ›å­¸ä¸­çš„ä¸ç¢ºå®šæ€§åŸç†æ˜¯ä»€éº¼ï¼Ÿè«‹çµ¦å‡ºæ•¸å­¸å¼èˆ‡ç‰©ç†æ„æ¶µã€‚ã€
- æ‡‰ç”¨æƒ…å¢ƒï¼šç”¨æ–¼æ¸¬è©¦ LLM æ˜¯å¦å…·å‚™é«˜éšå­¸è¡“ç†è§£èˆ‡æ¨ç†èƒ½åŠ›ã€‚

âœ… 2. BBHardï¼ˆBeyond the Benchmarks Hardï¼‰
- æ¸¬è©¦å…§å®¹ï¼šåŸæœ¬è¢«èªç‚ºè¶…å‡º LLM èƒ½åŠ›ç¯„åœçš„æ¨ç†èˆ‡ç†è§£ä»»å‹™ã€‚
- ä¾‹å­ï¼š
çµ¦ä¸€æ®µæŠ½è±¡ç§‘å¹»å°èªªï¼Œè¦æ±‚åˆ†æå…¶ç¤¾æœƒéš±å–»èˆ‡é‚è¼¯å‰æã€‚
- æ‡‰ç”¨æƒ…å¢ƒï¼šæª¢é©— Frontier LLM åœ¨å‰µé€ åŠ›èˆ‡é–‹æ”¾å¼å•é¡Œè™•ç†ä¸Šçš„æ¥µé™ã€‚

âœ… 3. Math Lv 5
- æ¸¬è©¦å…§å®¹ï¼šé«˜ä¸­æ•¸å­¸ç«¶è³½ç´šå•é¡Œï¼Œéœ€é«˜éšé‚è¼¯èˆ‡å…¬å¼é‹ç”¨ã€‚
- ä¾‹å­ï¼š
ã€Œè‹¥ä¸€å€‹å‡½æ•¸æ»¿è¶³ f(x+y)=f(x)f(y)ï¼Œä¸” f(0)=1ï¼Œæ±‚ f(2) çš„å¯èƒ½å€¼ã€‚
- æ‡‰ç”¨æƒ…å¢ƒï¼šå¯æ‡‰ç”¨æ–¼ AI æ•™å­¸è¼”åŠ©æˆ–å·¥ç¨‹å•é¡Œæ¨æ¼”ã€‚

âœ… 4. IFEvalï¼ˆInstruction Following Evaluationï¼‰
- æ¸¬è©¦å…§å®¹ï¼šæ¸¬è©¦æ¨¡å‹æ˜¯å¦èƒ½ç²¾æº–ä¾ç…§è¤‡é›œæŒ‡ä»¤ç”Ÿæˆçµæœã€‚
- ä¾‹å­ï¼š
æŒ‡ä»¤ï¼šã€Œå¯«ä¸€ç¯‡ä¸å°‘æ–¼ 400 å­—çš„çŸ­æ–‡ï¼Œä¸”è‡³å°‘ 3 æ¬¡æåˆ° AIï¼Œèªæ°£éœ€ç‚ºæ‡·èˆŠé¢¨æ ¼ã€‚ã€
- æ‡‰ç”¨æƒ…å¢ƒï¼šå°è©±ä»£ç†æˆ–ä»»å‹™å‹ AI çš„æ•ˆèƒ½è©•ä¼°ã€‚

âœ… 5. MuSRï¼ˆMultistep Soft Reasoningï¼‰
- æ¸¬è©¦å…§å®¹ï¼šå¤šæ­¥é©Ÿçš„æŸ”æ€§é‚è¼¯æ¨ç†ã€‚
- ä¾‹å­ï¼š
ã€Œé–±è®€ä¸€ç¯‡ 1000 å­—çš„æ¨ç†å°èªªç‰‡æ®µï¼Œæ¨æ–·èª°æ˜¯å…‡æ‰‹ä¸¦è§£é‡‹å…¶å‹•æ©Ÿèˆ‡æ‰‹æ®µã€‚ã€
- æ‡‰ç”¨æƒ…å¢ƒï¼šæ³•å¾‹åŠ©ç† AIã€è¨˜è€…åŠ©æ‰‹ã€åµæŸ¥åˆ†æå·¥å…·ç­‰ã€‚

âœ… 6. MMLU-PROï¼ˆMassive Multitask Language Understanding - PROç‰ˆï¼‰
- æ¸¬è©¦å…§å®¹ï¼šæ›´é«˜éšã€æ›´ä¹¾æ·¨çš„å¤šä»»å‹™ç†è§£æ¸¬è©¦ï¼Œç­”æ¡ˆé¸é …å¾ 4 å€‹æ“´å¢åˆ° 10 å€‹ã€‚
- ä¾‹å­ï¼š
ã€Œè«‹åœ¨ 10 å€‹é¸é …ä¸­é¸å‡ºæè¿°ç´°èƒåˆ†è£‚é€±æœŸé †åºæ­£ç¢ºçš„ä¸€é …ã€‚ã€
- æ‡‰ç”¨æƒ…å¢ƒï¼šé†«å­¸ã€å·¥ç¨‹ã€æ³•å­¸ã€æ•™è‚²ç­‰å°ˆæ¥­é ˜åŸŸçš„æ¨¡å‹èƒ½åŠ›é©—è­‰ã€‚

#### Hugging Face Open LLM Leaderboard
https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?official=true

#### What you can now do
- Code with Frontier Models including AI Assistants with Tools
- Build solutions with open-source LLMs with HuggingFace transformers
- Compare LLMs to identify the right one for the task at hand

### Week 4 Day 2
#### Learning Objectives
- Navigate the most useful leaderboards and Arenas to evaluate LLMs
- Give real-world use cases of LLMs solving commercial problems
- Confidently choose LLMs for your projects

#### Six Leaderboards
A tour of the essential leaderboards for selecting LLMs*
* **HuggingFace Open LLM**
  *New and old version*
  - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?official=true

* **HuggingFace BigCode**
  *Code generation*
  - https://huggingface.co/bigcode
* **HuggingFace LLM Perf**
* **HuggingFace Others**
  *e.g. Medical, Language-specific*
* **Vellum**
  *includes API cost and context window*
* **SEAL**
  *expert skills*

#### The Arena
https://lmarena.ai/?leaderboard

The LMSYS Chatbot Arena is an amazing resource
- Compare Frontier and Open-source models directly
- Blind human evals based on head-to-head comparison
- LLMs are measured with an 'ELO' rating

Participate in the voting - it's a terrific way to learn about different models while adding to the ecosystem

#### BigCode Leaderboard
https://www.bigcode-project.org/
https://huggingface.co/bigcode
- StarCoder2
- Code Llama2

| å°è±¡é¡åˆ¥           | ç”¨é€”èªªæ˜                                                                                         |
|:-------------------|:-------------------------------------------------------------------------------------------------|
| AI ç ”ç©¶äººå“¡        | ç ”ç©¶ç¨‹å¼ç¢¼ç”Ÿæˆã€è¨“ç·´æ•ˆç‡ã€æ¨¡å‹è§£é‡‹æ€§ç­‰ç›¸é—œä¸»é¡Œï¼Œä½¿ç”¨ StarCoder æ¨¡å‹èˆ‡ The Stack è³‡æ–™é›†é€²è¡Œå¯¦é©—ã€‚ |
| ç¨‹å¼é–‹ç™¼è€…èˆ‡å·¥ç¨‹å¸« | ä½¿ç”¨ StarCoder ç­‰æ¨¡å‹é€²è¡Œç¨‹å¼ç¢¼è£œå…¨ã€è‡ªå‹•æ¸¬è©¦ç”Ÿæˆã€ç¨‹å¼é‡æ§‹å»ºè­°ç­‰é–‹ç™¼å·¥ä½œã€‚                      |
| é–‹æºè²¢ç»è€…èˆ‡ç¤¾ç¾¤   | è²¢ç»é–‹æºæ¨¡å‹èˆ‡è³‡æ–™é›†ï¼Œå”åŠ©å»æ•èˆ‡è³‡æ–™æ¸…ç†ã€æå‡æ¨¡å‹å“è³ªã€‚                                         |
| æ•™è‚²èˆ‡å­¸è¡“æ©Ÿæ§‹     | ç”¨æ–¼æ•™æˆç”Ÿæˆå¼ AI èˆ‡å¤§å‹èªè¨€æ¨¡å‹çš„ç¨‹å¼ç¢¼æ‡‰ç”¨ï¼Œæ”¯æŒå­¸ç”Ÿå¯¦ä½œèˆ‡æ¢ç´¢ã€‚                               |
| ä¼æ¥­èˆ‡åˆå‰µå…¬å¸     | æ•´åˆè‡³å…§éƒ¨é–‹ç™¼æµç¨‹ï¼Œæ‰“é€ å®¢è£½åŒ–çš„ç¨‹å¼é–‹ç™¼åŠ©ç†èˆ‡è‡ªå‹•åŒ–å·¥å…·ã€‚                                       |

#### Commercial Use Cases
- Law 
    Harvey
- Talent 
    nebula.io
- Porting code
    Bloop (bloop.ai)
- Healthcare 
    Salesforce Health
- Education 
    Khanmigo

| æ‡‰ç”¨é ˜åŸŸ                   | å…¬å¸ / ç”¢å“               | èªªæ˜                                           |
| ---------------------- | --------------------- | -------------------------------------------- |
| âš–ï¸ æ³•å¾‹ï¼ˆLawï¼‰             | **Harvey**            | ç‚ºå¾‹å¸«æˆ–æ³•å¾‹äº‹å‹™æ‰€æä¾› AI æ³•å¾‹åŠ©ç†ï¼Œä¾‹å¦‚è‡ªå‹•è‰æ“¬åˆç´„ã€ç¸½çµæ¡ˆä¾‹ã€å›ç­”æ³•å¾‹å•é¡Œ     |
| ğŸŒŸ äººæ‰æ‹›å‹Ÿï¼ˆTalentï¼‰        | **nebula.io**         | å”åŠ© HR æˆ–çµé ­å¿«é€Ÿåˆ†æå±¥æ­·ã€æ¨è–¦è·ç¼ºã€æ’°å¯«æ‹›å‹Ÿä¿¡æˆ–é€²è¡Œå€™é¸äººç¯©é¸           |
| ğŸ’» ç¨‹å¼è½‰æ›ï¼ˆPorting Codeï¼‰  | **Bloop**             | å”åŠ©é–‹ç™¼è€…é–±è®€èˆŠç¨‹å¼ç¢¼ã€è½‰æ›èªè¨€ï¼ˆå¾ COBOL åˆ° Javaï¼‰ã€ç”Ÿæˆæ–‡ä»¶èˆ‡å–®å…ƒæ¸¬è©¦ |
| â¤ï¸â€ğŸ©¹ é†«ç™‚ä¿å¥ï¼ˆHealthcareï¼‰ | **Salesforce Health** | æä¾› AI é†«ç™‚åŠ©ç†ï¼Œä¾‹å¦‚ç—…æ­·æ‘˜è¦ã€è‡ªå‹•å›ç­”ç—…äººæŸ¥è©¢ã€è‡¨åºŠæ±ºç­–æ”¯æ´            |
| ğŸ“ æ•™è‚²ï¼ˆEducationï¼‰       | **Khanmigo**          | ç”±å¯æ±—å­¸é™¢é–‹ç™¼ï¼Œæä¾›å­¸ç”Ÿ AI æ•™å­¸åŠ©ç†ï¼Œèƒ½å³æ™‚è§£é‡‹æ¦‚å¿µã€æ‰¹æ”¹ä½œæ¥­æˆ–æ¨¡æ“¬è€å¸«è§’è‰²     |

#### BigCode Leaderboard
https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard

#### LLM Perf Leaderboard
https://huggingface.co/spaces/optimum/llm-perf-leaderboard
https://huggingface.co/spaces/optimum/llm-perf-leaderboard#/?official=true

#### HuggingFace Spaces Leaderboard
https://huggingface.co/spaces?q=Leaderboard

#### Open Medical LLMs Leaderboard
https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard

#### Vellum Leaderboard
https://www.vellum.ai/llm-leaderboard#compare
https://www.vellum.ai/llm-leaderboard#/?official=true

#### Chatbot Arena (Human Eval)
https://lmarena.ai/?leaderboard&gad_source=1&gad_campaignid=21946979971&gbraid=0AAAAA-d12XORV-SaxIcoX0ZeRT4lziwkW&gclid=Cj0KCQjwrPHABhCIARIsAFW2XBOMPw3jS-oz9OxTULiQwBdbhMKA-HtA9szFVmRnq7BHns_4A96jO_saAj7oEALw_wcB

- Language Models
    Category
- Overview
- Price Analysis

- Vote
    https://lmarena.ai/

#### Other AIs solutions
| Website              | AI Use Case                                          |
| -------------------- | ---------------------------------------------------- |
| **Harvey AI**        | https://www.harvey.ai Legal assistant for law firms and corporate counsel  |
| **Nebula.io**        | Knowledge AI trained on personal docs                |
| **Bloop.ai**         | AI code search and understanding for devs            |
| **Einstein Copilot** | CRM assistant for business productivity (Salesforce) https://www.salesforce.com/news/press-releases/2024/02/27/einstein-copilot-news/ |
| **Khanmigo.ai**      | AI tutor for students and teachers                   |

#### BUSINESS CHALLENGE
Introducing our commercial challenge this week

- Build a product that converts Python code to C++ for performance
    Solution with a Frontier model
    Solution with an Open-Source model

Let's start by selecting the LLMs most suited for the task

#### What you can now do
- Code with Frontier Models including AI Assistants with Tools
- Build solutions with open-source LLMs with HuggingFace transformers (i.e. from transformers import pipeline, tokenizer, model)
- Confidently choose the right LLM for your project, backed by metrics (Leaderboards, Benchmarks, ELO ratings)
